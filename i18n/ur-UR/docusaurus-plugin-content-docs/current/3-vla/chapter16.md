---
id: chapter16
title: Module-3.vla-chapter-16
sidebar_position: 4
slug: /vla/chapter16
---

# باب 16: ویژن اور زبان سے ایکشن پالیسیاں سیکھنا

ایک روبوٹ کے لسانی تصورات کو بصری طور پر گراؤنڈ کرنے اور اپنے ماحول کو سمجھنے کے بعد، ویژن-لینگویج-ایکشن (VLA) سسٹم میں اگلا اہم مرحلہ اس سمجھ کو جسمانی اعمال میں ترجمہ کرنا ہے۔ اس میں **ایکشن پالیسیاں** سیکھنا شامل ہے جو یہ بتاتی ہیں کہ روبوٹ کو قدرتی زبان میں بیان کردہ مطلوبہ مقصد کو حاصل کرنے کے لیے دنیا کے ساتھ کیسے حرکت کرنا اور تعامل کرنا چاہیے۔ یہ باب ان پالیسیوں کو سیکھنے کے مختلف طریقوں کو تلاش کرتا ہے۔

## VLA میں ایکشن کا مسئلہ

VLA میں ایکشن کا مسئلہ ایک سمجھی ہوئی ماحولیاتی حالت (ویژن سے) اور ایک اعلیٰ سطحی لسانی مقصد (زبان سے) کو کم سطحی موٹر کمانڈز کی ترتیب میں نقشہ بنانا ہے۔ یہ کئی وجوہات کی بنا پر مشکل ہے:

*   **اعلی جہتی ایکشن اسپیسز**: روبوٹس میں اکثر کئی ڈگری آف فریڈم ہوتی ہیں، جو بڑی تعداد میں ممکنہ حرکات کا باعث بنتی ہیں۔
*   **طویل افق کے کام**: پیچیدہ کاموں کے لیے کئی ترتیب وار اعمال کی ضرورت ہوتی ہے، جو براہ راست اینڈ-ٹو-اینڈ لرننگ کو مشکل بنا دیتے ہیں۔
*   **عمومیت**: پالیسیاں نئے اشیاء، ماحول، اور ہدایات میں تغیرات کے لیے عام ہونی چاہئیں۔
*   **حفاظت اور حقیقی دنیا کی رکاوٹیں**: اعمال جسمانی طور پر قابل عمل، محفوظ، اور ماحولیاتی قواعد کی پابندی کرنے والے ہونے چاہئیں۔

## ایکشن پالیسیاں سیکھنے کے طریقے

### 1. ایمیٹیشن لرننگ (مظاہروں سے سیکھنا - LfD)

ایمیٹیشن لرننگ ایک عام اور بدیہی طریقہ ہے جہاں ایک روبوٹ انسانی مظاہروں کو دیکھ کر سیکھتا ہے۔ بنیادی خیال ماہر کے رویے کی نقل کرنے والی پالیسی سیکھنا ہے۔

*   **یہ کیسے کام کرتا ہے**: ایک انسان روبوٹ کو ٹیلی آپریٹ کرتا ہے یا کوئی کام انجام دیتا ہے، اور روبوٹ سینسر ڈیٹا (ویژن) اور متعلقہ موٹر کمانڈز (اعمال) کو ریکارڈ کرتا ہے۔ جمع کردہ ڈیٹا کو پھر ایک پالیسی (مثلاً، ایک نیورل نیٹ ورک) کو تربیت دینے کے لیے استعمال کیا جاتا ہے تاکہ بصری حالتوں کو اعمال میں نقشہ بنایا جا سکے۔
*   **VLA انضمام**: انسانی مظاہروں کو کام یا مخصوص ذیلی اہداف کی قدرتی زبان کی تفصیلات کے ساتھ بڑھایا جا سکتا ہے، جس سے روبوٹ کو زبان پر مبنی پالیسیاں سیکھنے کی اجازت ملتی ہے۔
*   **فوائد**: ڈیٹا جمع کرنا نسبتاً آسان ہے، پیچیدہ کاموں کے لیے پالیسیاں سیکھی جا سکتی ہیں۔
*   **چیلنجز**: **کوویریٹ شفٹ** (روبوٹ مظاہرہ شدہ حالتوں سے ہٹ جاتا ہے)، بہت زیادہ اعلیٰ معیار کے انسانی ڈیٹا کی ضرورت ہوتی ہے، انسانی اور روبوٹ کی حالتوں کے درمیان **تقسیم کی عدم مطابقت**۔

### 2. رینفورسمنٹ لرننگ (RL)

رینفورسمنٹ لرننگ میں ایک ایجنٹ ماحول میں آزمائش اور غلطی کے ذریعے بہترین رویہ سیکھتا ہے، جو انعامی سگنلز کے ذریعے چلایا جاتا ہے۔

*   **یہ کیسے کام کرتا ہے**: روبوٹ (ایجنٹ) اعمال انجام دیتا ہے، نتیجے میں آنے والی حالت کا مشاہدہ کرتا ہے، اور ایک انعام وصول کرتا ہے۔ وقت کے ساتھ، یہ ایک ایسی پالیسی سیکھتا ہے جو مجموعی انعام کو زیادہ سے زیادہ کرتی ہے۔
*   **VLA انضمام**: انعامی فنکشن کو لسانی اہداف کو شامل کرنے کے لیے ڈیزائن کیا جا سکتا ہے (مثلاً، "سرخ بلاک" کے طور پر بیان کردہ چیز تک پہنچنے کے لیے زیادہ انعام)۔ قدرتی زبان کی ہدایات بھی ہدف کی حالت کے طور پر کام کر سکتی ہیں یا پالیسی کو مشروط کر سکتی ہیں۔
*   **فوائد**: نئے اور بہترین رویے دریافت کر سکتا ہے، انسانی مظاہروں پر کم انحصار کرتا ہے۔
*   **چیلنجز**: **سیمپل ایفیشنسی** (بہت زیادہ تعاملات کی ضرورت ہوتی ہے، اکثر سمولیشن میں)، **ریوارڈ انجینئرنگ** (مؤثر انعامی فنکشنز ڈیزائن کرنا مشکل ہے)، **سم-ٹو-ریئل گیپ**۔

### 3. زبان پر مبنی پالیسیاں

یہ طریقہ واضح طور پر زبان کو پالیسی کے ان پٹ کے طور پر ضم کرتا ہے۔ روبوٹ مختلف لسانی کمانڈز کی بنیاد پر مختلف اعمال کو انجام دینا سیکھتا ہے۔

*   **فن تعمیر**: اکثر بصری ان پٹ اور زبان ان پٹ کے لیے الگ انکوڈرز کے ساتھ ایک نیورل نیٹ ورک شامل ہوتا ہے۔ انکوڈ شدہ خصوصیات کو فیوز کیا جاتا ہے اور ایک ایکشن آؤٹ پٹ لیئر کو دیا جاتا ہے۔
*   **تربیت**: ایسے ڈیٹا سیٹس پر تربیت یافتہ ہوتا ہے جہاں بصری مشاہدات، لسانی ہدایات، اور متعلقہ اعمال فراہم کیے جاتے ہیں۔
*   **فوائد**: زبان کی بنیاد پر بہترین کنٹرول اور عمومیت کو قابل بناتا ہے، روبوٹس کو انتہائی ہدایت پر عمل کرنے والا بناتا ہے۔
*   **چیلنجز**: جوڑی شدہ ویژن-لینگویج-ایکشن ڈیٹا سیٹس کی ضرورت ہوتی ہے، بصری اور لسانی خصوصیات کو ایکشن اسپیس سے جوڑنا۔

### 4. LLMs کے ساتھ درجہ بندی کی منصوبہ بندی

بڑے لینگویج ماڈلز (LLMs) کو اعلیٰ سطحی منصوبے بنانے کے لیے استعمال کیا جا سکتا ہے، جنہیں پھر کم سطحی، سیکھی ہوئی روبوٹ پالیسیاں انجام دیتی ہیں۔

*   **یہ کیسے کام کرتا ہے**: صارف ایک LLM کو اعلیٰ سطحی ہدایت دیتا ہے۔ LLM اسے خلاصہ ذیلی اہداف یا روبوٹ API فنکشنز (ٹولز) کی ترتیب میں ترجمہ کرتا ہے۔ ہر خلاصہ ذیلی ہدف کو پھر ایک خصوصی، سیکھی ہوئی کم سطحی پالیسی (مثلاً، ایک "اٹھاؤ" پالیسی، ایک "رکھو" پالیسی، ایک "نیویگیٹ" پالیسی) سنبھالتی ہے۔
*   **VLA انضمام**: LLM زبان کی سمجھ اور اعلیٰ سطحی استدلال کو سنبھالتا ہے، جبکہ کم سطحی پالیسیاں بصری پرسیپشن اور موٹر کنٹرول کو سنبھالتی ہیں۔
*   **فوائد**: LLMs کی استدلال کی طاقت کو سیکھی ہوئی پالیسیوں کی مضبوطی کے ساتھ جوڑتا ہے؛ طویل افق کے کاموں کو مؤثر طریقے سے سنبھالتا ہے۔
*   **چیلنجز**: کم سطحی مہارتوں/ٹولز کا ایک اچھا سیٹ متعین کرنا، LLM اور روبوٹ APIs کے درمیان انٹرفیس کا انتظام کرنا۔

## مصنوعی ڈیٹا اور سم-ٹو-ریئل منتقلی

لرننگ پر مبنی طریقوں کی ڈیٹا کی ضروریات کو دیکھتے ہوئے، آئزک سم جیسے اعلیٰ وفاداری والے سمیلیٹروں میں **مصنوعی ڈیٹا کی پیداوار** سب سے اہم ہے۔ روبوٹس کو متنوع نقلی ماحول میں تربیت دی جا سکتی ہے، جس میں مصنوعی سینسر ڈیٹا فراہم کرتے ہیں، اور پھر ان سیکھی ہوئی پالیسیوں کو حقیقی دنیا میں منتقل کیا جا سکتا ہے (**سم-ٹو-ریئل منتقلی**)۔

*   **ڈومین رینڈمائزیشن**: حقیقی دنیا کی تغیرات کے لیے پالیسی کی مضبوطی کو بہتر بنانے کے لیے سمولیشن میں ٹیکسچرز، روشنی، اشیاء کی پوزیشنز، اور کیمرہ پیرامیٹرز کو رینڈم کرنا۔
*   **ڈومین ایڈاپٹیشن**: سمولیشن میں تربیت یافتہ پالیسیوں کو حقیقی دنیا میں اچھی کارکردگی دکھانے کے لیے ڈھالنے کی تکنیکیں۔

اس باب نے ایکشن پالیسیاں سیکھنے کے مختلف طریقوں کو تلاش کیا ہے جو روبوٹس کو بصری اور لسانی سمجھ کو جسمانی اعمال میں ترجمہ کرنے کے قابل بناتے ہیں۔ ایمیٹیشن اور رینفورسمنٹ لرننگ سے لے کر زبان پر مبنی کنٹرول اور LLMs کے ساتھ درجہ بندی کی منصوبہ بندی تک، یہ تکنیکیں حقیقی معنوں میں ذہین اور ورسٹائل روبوٹک سسٹمز کی ترقی کو آگے بڑھا رہی ہیں۔ یہ ویژن-لینگویج-ایکشن (VLA) ماڈیول کا اختتام کرتا ہے۔ اگلے ماڈیول میں، ہم ایک جامع کیپ اسٹون پروجیکٹ میں ان تصورات کو لاگو کریں گے۔
